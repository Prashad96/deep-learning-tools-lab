{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Deep Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH/ZtfvtlhQU1cQeukmuFB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/livinNector/deep-learning-tools-lab/blob/main/3%20-%20Deep%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Deep Neural Networks"
      ],
      "metadata": {
        "id": "Hy5B8pkrDTRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using Deep Neural Networks"
      ],
      "metadata": {
        "id": "OTi3EhvxDgTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "J0WwKHQcErMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Used"
      ],
      "metadata": {
        "id": "wfUDdYtHD59_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Titanic Dataset : https://www.tensorflow.org/datasets/catalog/titanic"
      ],
      "metadata": {
        "id": "V9BD_qXnEBPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset"
      ],
      "metadata": {
        "id": "YnZ10qV2ElSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ],
      "metadata": {
        "id": "PrVtyXnAFViS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(titanic_file_path)"
      ],
      "metadata": {
        "id": "HeqLJHjHrHVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "d9mehFbmrJiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns = {\"survived\":\"target\"},inplace=True)"
      ],
      "metadata": {
        "id": "YmsM_U_OrOZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(5)\n",
        "train, val, test = np.split(df.sample(frac=1), [int(0.8*len(df)), int(0.9*len(df))])"
      ],
      "metadata": {
        "id": "A-FUlowmsYjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "KtRkbAjnsjoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  df = dataframe.copy()\n",
        "  labels = df.pop('target')\n",
        "  df = {key: value.values[:,tf.newaxis] for key, value in dataframe.items()}\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "tXBvXTObmHul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "fKqfXjvDsP1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the dataset"
      ],
      "metadata": {
        "id": "UpWegJf6GiMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "1. Classifying Structured data using keras preprocessing layers - https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n",
        "2. keras preprocessing layers - https://www.tensorflow.org/guide/keras/preprocessing_layers\n",
        "\n",
        "The titanic dataset is preprocessed using the following keras preprocessing layers:\n",
        "- `tf.keras.layers.Normalization`\n",
        "- `tf.keras.layers.CategoryEncoding`\n",
        "- `tf.keras.layers.StringLookup`\n",
        "- `tf.keras.layers.IntegerLookup`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sBOtOwmlGrha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for the feature.\n",
        "  normalizer = tf.keras.layers.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer\n"
      ],
      "metadata": {
        "id": "VKapMftwG7Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a layer that turns strings into integer indices.\n",
        "  if dtype == 'string':\n",
        "    index = tf.keras.layers.StringLookup(max_tokens=max_tokens)\n",
        "  # Otherwise, create a layer that turns integer values into integer indices.\n",
        "  else:\n",
        "    index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)\n",
        "\n",
        "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Encode the integer indices.\n",
        "  encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
        "\n",
        "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
        "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
        "  return lambda feature: encoder(index(feature))\n"
      ],
      "metadata": {
        "id": "9F_fL-pKlvG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = [\"age\",\"fare\"]\n",
        "numerical_categorical_cols = [\"n_siblings_spouses\",\"parch\"]\n",
        "categorical_cols = [\"sex\",\"class\",\"deck\",\"embark_town\",\"alone\"]\n",
        "\n",
        "all_inputs = []\n",
        "encoded_features = []\n",
        "\n",
        "# Numerical features.\n",
        "for header in numerical_cols:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds) # Normalization\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs.append(numeric_col)\n",
        "  encoded_features.append(encoded_numeric_col)\n",
        "\n",
        "# Numerical Categorial features\n",
        "for header in numerical_categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
        "  encoding_layer = get_category_encoding_layer(name=header,dataset=train_ds,dtype='int64') # encoding\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n",
        "\n",
        "# Other categorical Features\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(name=header,dataset=train_ds,dtype='string',max_tokens=5) # encoding\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n"
      ],
      "metadata": {
        "id": "rtqf0Y3FoRri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the model"
      ],
      "metadata": {
        "id": "XIDkFIZgwb9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.concatenate(encoded_features)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(4, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(2, activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(all_inputs,outputs)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "lp7cBZAnvaBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "DLR-XRSTxBaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "Y6tFBRkl4dKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds,validation_data=val_ds,epochs=200)"
      ],
      "metadata": {
        "id": "NC7gkfVW4fuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ploting learing curves"
      ],
      "metadata": {
        "id": "p55M0Sj540pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = history.history"
      ],
      "metadata": {
        "id": "y47Yge886Jbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history[\"accuracy\"],label=\"train acc\")\n",
        "plt.plot(history[\"val_accuracy\"],label=\"val acc\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history[\"loss\"],label=\"train loss\")\n",
        "plt.plot(history[\"val_loss\"],label=\"val loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4izsZIX66FDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model on test set"
      ],
      "metadata": {
        "id": "o5K6Yrel7FG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss ,accuracy = model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "TZa0DugZ7IUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test loss :\",loss)\n",
        "print(\"test accuracy :\",accuracy)"
      ],
      "metadata": {
        "id": "2uQ0kUZV7KIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}